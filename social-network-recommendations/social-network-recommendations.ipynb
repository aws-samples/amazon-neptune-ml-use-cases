{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Social Network Recommendations\n",
    "\n",
    "In this example, we're going to build a powerful social network predictive capability with Netpune ML. The techniques introduced here can be used to build predictions in other domains outside of social networks.\n",
    "\n",
    "You can quickly setup the environment by using the Neptune ML AWS CloudFormation template: \n",
    "https://docs.aws.amazon.com/neptune/latest/userguide/machine-learning-quick-start.html\n",
    "\n",
    "This code is extended base on Neptune ML code samples preconfigured using above CloudFormation. \n",
    "\n",
    "### People You May Know\n",
    "\n",
    "Recommender systems are one of most widely adopted machine learning technologies in real world applications, ranging from social network to e-commerce platforms. In social network, one common use case is to recommend new friends to a user, based on user’s friendship with the others. Users that have common friends are likely to know each other, thus should have a higher score for recommendation system to propose if they are not yet connected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Before we begin, we'll clear any existing data from our Neptune cluster, using the cell magic `%%gremlin` and a subsequent drop query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%gremlin\n",
    "\n",
    "g.V().drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we know which Neptune cluster to access? The cell magics exposed by Neptune Notebooks use a configuration located by default under `~/graph_notebook_config.json` At the time of initialization of the Sagemaker instance, this configuration is generated using environment variables derived from the cluster being connected to. \n",
    "\n",
    "You can check the contents of the configuration in two ways. You can print the file itself, or you can look for the configuration being used by the notebook which you have opened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cat ~/graph_notebook_config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Social Network\n",
    "\n",
    "Next, we'll create a small social network. Note that the script below comprises a single statement. All the vertices and edges here will be created in the context of a single transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%gremlin\n",
    "\n",
    "g.\n",
    "addV('User').property('name','Bill').property('interests', 'arts;comics;games;sports').\n",
    "addV('User').property('name','Sarah').property('interests', 'arts').\n",
    "addV('User').property('name','Ben').property('interests', 'electronics').\n",
    "addV('User').property('name','Lucy').property('interests', 'electronics').\n",
    "addV('User').property('name','Colin').property('interests', 'games;sports').\n",
    "addV('User').property('name','Emily').property('interests', 'sports').\n",
    "addV('User').property('name','Gordon').property('interests', 'sports').\n",
    "addV('User').property('name','Kate').property('interests', 'arts').\n",
    "addV('User').property('name','Peter').property('interests', 'games').\n",
    "addV('User').property('name','Terry').property('interests', 'sports').\n",
    "addV('User').property('name','Alistair').property('interests', 'arts;sports').\n",
    "addV('User').property('name','Eve').property('interests', 'arts;electronics').\n",
    "addV('User').property('name','Gary').property('interests', 'sports').\n",
    "addV('User').property('name','Mary').property('interests', 'comics;games').\n",
    "addV('User').property('name','Charlie').property('interests', 'games;electronics').\n",
    "addV('User').property('name','Sue').property('interests', 'electronics').\n",
    "addV('User').property('name','Arnold').property('interests', 'comics;games').\n",
    "addV('User').property('name','Chloe').property('interests', 'sports').\n",
    "addV('User').property('name','Henry').property('interests', 'arts;comics;games').\n",
    "addV('User').property('name','Josie').property('interests', 'electronics').\n",
    "V().hasLabel('User').has('name','Sarah').as('a').V().hasLabel('User').has('name','Bill').addE('FRIEND').to('a').\n",
    "V().hasLabel('User').has('name','Colin').as('a').V().hasLabel('User').has('name','Bill').addE('FRIEND').to('a').\n",
    "V().hasLabel('User').has('name','Terry').as('a').V().hasLabel('User').has('name','Bill').addE('FRIEND').to('a').\n",
    "V().hasLabel('User').has('name','Peter').as('a').V().hasLabel('User').has('name','Colin').addE('FRIEND').to('a').\n",
    "V().hasLabel('User').has('name','Kate').as('a').V().hasLabel('User').has('name','Ben').addE('FRIEND').to('a').\n",
    "V().hasLabel('User').has('name','Kate').as('a').V().hasLabel('User').has('name','Lucy').addE('FRIEND').to('a').\n",
    "V().hasLabel('User').has('name','Eve').as('a').V().hasLabel('User').has('name','Lucy').addE('FRIEND').to('a').\n",
    "V().hasLabel('User').has('name','Alistair').as('a').V().hasLabel('User').has('name','Kate').addE('FRIEND').to('a').\n",
    "V().hasLabel('User').has('name','Gary').as('a').V().hasLabel('User').has('name','Colin').addE('FRIEND').to('a').\n",
    "V().hasLabel('User').has('name','Gordon').as('a').V().hasLabel('User').has('name','Emily').addE('FRIEND').to('a').\n",
    "V().hasLabel('User').has('name','Alistair').as('a').V().hasLabel('User').has('name','Emily').addE('FRIEND').to('a').\n",
    "V().hasLabel('User').has('name','Terry').as('a').V().hasLabel('User').has('name','Gordon').addE('FRIEND').to('a').\n",
    "V().hasLabel('User').has('name','Alistair').as('a').V().hasLabel('User').has('name','Terry').addE('FRIEND').to('a').\n",
    "V().hasLabel('User').has('name','Gary').as('a').V().hasLabel('User').has('name','Terry').addE('FRIEND').to('a').\n",
    "V().hasLabel('User').has('name','Mary').as('a').V().hasLabel('User').has('name','Terry').addE('FRIEND').to('a').\n",
    "V().hasLabel('User').has('name','Henry').as('a').V().hasLabel('User').has('name','Alistair').addE('FRIEND').to('a').\n",
    "V().hasLabel('User').has('name','Sue').as('a').V().hasLabel('User').has('name','Eve').addE('FRIEND').to('a').\n",
    "V().hasLabel('User').has('name','Sue').as('a').V().hasLabel('User').has('name','Charlie').addE('FRIEND').to('a').\n",
    "V().hasLabel('User').has('name','Josie').as('a').V().hasLabel('User').has('name','Charlie').addE('FRIEND').to('a').\n",
    "V().hasLabel('User').has('name','Henry').as('a').V().hasLabel('User').has('name','Charlie').addE('FRIEND').to('a').\n",
    "V().hasLabel('User').has('name','Henry').as('a').V().hasLabel('User').has('name','Mary').addE('FRIEND').to('a').\n",
    "V().hasLabel('User').has('name','Mary').as('a').V().hasLabel('User').has('name','Gary').addE('FRIEND').to('a').\n",
    "V().hasLabel('User').has('name','Henry').as('a').V().hasLabel('User').has('name','Gary').addE('FRIEND').to('a').\n",
    "V().hasLabel('User').has('name','Chloe').as('a').V().hasLabel('User').has('name','Gary').addE('FRIEND').to('a').\n",
    "V().hasLabel('User').has('name','Henry').as('a').V().hasLabel('User').has('name','Arnold').addE('FRIEND').to('a').\n",
    "next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the network looks like:\n",
    "    \n",
    "<img src=\"https://s3.amazonaws.com/aws-neptune-customer-samples/neptune-sagemaker/images/03-social-network.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the number of users in the graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%gremlin\n",
    "g.V().groupCount().by(label).unfold()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the number of relations among users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%gremlin\n",
    "g.E().groupCount().by(label).unfold()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Henry's friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%gremlin\n",
    "\n",
    "g.V().hasLabel('User').has('name', 'Henry').both('FRIEND').groupCount().by('name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Henry's interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%gremlin\n",
    "\n",
    "g.V().hasLabel('User').has('name', 'Henry').values('interests')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a recommendation by simple query\n",
    "\n",
    "Let's now create a simple query to recommend for a specific user.\n",
    "\n",
    "In the query below, we're finding the vertex that represents our user. We're then traversing `FRIEND` relationships (we don't care about relationship direction, so we're using `both()`) to find that user's immediate friends. We're then traversing another hop into the graph, looking for friends of those friends who _are not currently connected to our user.\n",
    "\n",
    "We then count the paths to these candidate friends, and order the results based on the number of times we can reach a candidate via one of the user's immediate friends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%gremlin\n",
    "\n",
    "g.V().hasLabel('User').has('name', 'Henry').as('user').  \n",
    "  both('FRIEND').aggregate('friends').  \n",
    "  both('FRIEND').\n",
    "    where(P.neq('user')).where(P.without('friends')).  \n",
    "  groupCount().by('name').  \n",
    "  order(Scope.local).by(values, Order.decr).\n",
    "  next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your Graph Convolution Network with Amazon Neptune ML\n",
    "\n",
    "Neptune ML uses graph neural network technology to automatically creates, trains, and applies ML models on your graph data. Neptune ML supports common graph prediction tasks such as node classification, node regression, edge classification and regression, and link prediction. \n",
    "It is powered by: \n",
    "- **Amazon Neptune:** a purpose-built, high-performance managed graph database, which is optimized for storing billions of relationships and querying the graph with milliseconds latency. Learn more at Overview of Amazon Neptune Features.\n",
    "- **Amazon SageMaker:** a fully managed service that provides every developer and data scientist with the ability to prepare build, train, and deploy machine learning (ML) models quickly. \n",
    "- **Deep Graph Library (DGL):** an open-source, high performance and scalable Python package for deep learning on graphs. It provides fast and memory-efficient message passing primitives for training Graph Neural Networks. Neptune ML uses DGL to automatically choose and train the best ML model for your workload, enabling you to make ML-based predictions on graph data in hours instead of weeks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data export and configuration\n",
    "\n",
    "The first step in our Neptune ML process is to export the graph data from the Neptune Cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup for S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket_uri=\"s3://(put-your-bucket-name-here-****)/neptune-ml-social-network-recommendation/\"\n",
    "# remove trailing slashes\n",
    "s3_bucket_uri = s3_bucket_uri[:-1] if s3_bucket_uri.endswith('/') else s3_bucket_uri\n",
    "s3_bucket_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME_DIRECTORY = '~'\n",
    "\n",
    "import os \n",
    "import json\n",
    "import logging\n",
    "def load_configuration():\n",
    "    with open(os.path.expanduser(f'{HOME_DIRECTORY}/graph_notebook_config.json')) as f:\n",
    "        data = json.load(f)\n",
    "        host = data['host']\n",
    "        port = data['port']\n",
    "        if data['auth_mode'] == 'IAM':\n",
    "            iam = True\n",
    "        else:\n",
    "            iam = False\n",
    "    return host, port, iam\n",
    "\n",
    "\n",
    "def get_host():\n",
    "    host, port, iam = load_configuration()\n",
    "    return host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neptune_host = get_host()\n",
    "neptune_host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "def get_export_service_host():\n",
    "    with open(os.path.expanduser(f'{HOME_DIRECTORY}/.bashrc')) as f:\n",
    "        data = f.readlines()\n",
    "        print(data)\n",
    "    for d in data:\n",
    "        if str.startswith(d, 'export NEPTUNE_EXPORT_API_URI'):\n",
    "            parts = d.split('=')\n",
    "            if len(parts) == 2:\n",
    "                path = urlparse(parts[1].rstrip())\n",
    "                return path.hostname + \"/v1\"\n",
    "    logging.error(\n",
    "        \"Unable to determine the Neptune Export Service Endpoint. You will need to enter this or assign it manually.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### export_params\n",
    "\n",
    "The first step in our Neptune ML process is to export the graph data from the Neptune Cluster. To do so, we need to specify the parameters for the data export and model configuration. Here is our example of export parameters. \n",
    "\n",
    "In export_params, we need to configure the basic setup such as the neptune host and output S3 path for exported data storage. The configuration specified in additionalParams is the type of machine learning task to perform. In this example, link prediction is optionally used to predict a particular edge type (User—FRIEND—User). If no target type is specified, Neptune ML will assume that the task is Link Prediction. The parameters also specify details about the data stored in our graph and how the machine learning model will interpret that data (we have “User” as node, and node property as “interests”). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_params={ \n",
    "\"command\": \"export-pg\", \n",
    "\"params\": { \"endpoint\": neptune_host,\n",
    "            \"profile\": \"neptune_ml\",\n",
    "            \"cloneCluster\": False\n",
    "            }, \n",
    "\"outputS3Path\": f'{s3_bucket_uri}/neptune-export',\n",
    "\"additionalParams\": {\n",
    "        \"neptune_ml\": {\n",
    "          \"version\": \"v2.0\",\n",
    "        \"targets\": [\n",
    "            {\n",
    "                \"edge\": [\"User\", \"FRIEND\", \"User\"],\n",
    "                \"type\" : \"link_prediction\"\n",
    "            }\n",
    "         ],\n",
    "         \"features\": [\n",
    "            {\n",
    "                \"node\": \"User\",\n",
    "                \"property\": \"interests\",\n",
    "                \"type\": \"category\",\n",
    "                \"separator\": \";\"\n",
    "            }\n",
    "         ]\n",
    "        }\n",
    "      },\n",
    "\"jobSize\": \"small\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%neptune_ml export start --export-url {get_export_service_host()} --export-iam --wait --store-to export_results\n",
    "${export_params}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the export job succeed, we will have the Neptune graph DB exported into CSV format and stored in an S3 bucket. There will be two types of files; nodes.csv and edges.csv. training-data-configuration.json: will also be generated which has configuration needed for Neptune ML to do model training. See [export data from Neptune for Neptune ML](https://docs.aws.amazon.com/neptune/latest/userguide/machine-learning-data-export.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "\n",
    "Neptune ML performs feature extraction and encoding as part of the data-processing steps. Common types of pre-processing of properties are: encoding categorical features through one-hot encoding, bucketing numerical features, or using word2vec to encode a string property or other free-form text property values.\n",
    "\n",
    "In our example, we will simply use the property “interests”. Neptune ML encodes the values as multi-categorical. However, if such categorical value is complex, i.e. more than 3 words per node. Neptune ML infers the property type to be text and uses the text_word2vec encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training_job_name can be set to a unique value below, otherwise one will be auto generated\n",
    "import time \n",
    "processing_job_name=f'social-link-prediction-processing-{int(time.time())}'\n",
    "\n",
    "processing_params = f\"\"\"\n",
    "--config-file-name training-data-configuration.json\n",
    "--job-id {processing_job_name} \n",
    "--s3-input-uri {export_results['outputS3Uri']} \n",
    "--s3-processed-uri {str(s3_bucket_uri)}/preloading \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%neptune_ml dataprocessing start --wait --store-to processing_results {processing_params}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of this step, a DGL (Deep Graph library) graph is generated from the exported dataset for the model training step to use. Neptune ML automatically tune the model with Hyperparameter Optimization Tuning jobs defined in training-data-configuration.json. We can download and modify this file to tune the model’s hyperparameters, such as batch-size, num-hidden, num-epochs, dropout etc. \n",
    "\n",
    "See [Processing the graph data exported from Neptune for training](https://docs.aws.amazon.com/neptune/latest/userguide/machine-learning-on-graphs-processing.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "The next step in the process is the automated training of the GNN model. The model training is done in two stages. The first stage uses a SageMaker Processing job to generate a model training strategy — a configuration set that specifies what type of model and model hyperparameter ranges will be used for the model training. \n",
    "Then, SageMaker hyperparameter tuning job will be launched. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important ! Change the batch size \n",
    "\n",
    "Neptune ML automatically tune the model with Hyperparameter Optimization Tuning jobs defined in training-data-configuration.json. Customer has the possibility to modify this file to tune the model according to the given parameters, such as batch_size, num-hidden, num-epochs, dropout etc. \n",
    "\n",
    "We illustrate how to change batch size. In our unconventional tiny network example here, it is required to change the batch size to prevent training job failure.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prcossing_location = processing_results['processingJob']['outputLocation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name, key_name = prcossing_location.replace(\"s3://\", \"\").split(\"/\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "s3.download_file(bucket_name,key_name + '/model-hpo-configuration.json','model-hpo-configuration.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat model-hpo-configuration.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace batch-size as our network size is tiny \n",
    "                {\n",
    "                    \"param\": \"batch-size\",\n",
    "                    \"range\": [\n",
    "                        2,\n",
    "                        4\n",
    "                    ],\n",
    "                    \"inc_strategy\": \"power2\",\n",
    "                    \"type\": \"int\",\n",
    "                    \"default\": 2\n",
    "                },"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.upload_file('model-hpo-configuration.json', bucket_name, key_name + '/model-hpo-configuration.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SageMaker Hyperparameter Tuning Optimization job runs a pre-specified number of model training job trials on the processed data, try different hyperparameter combinaisons according to **model-hpo-configuration.json**, and stores the model artifacts generated by the training in the output S3 location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_name=f'social-link-prediction-{int(time.time())}'\n",
    "\n",
    "training_params=f\"\"\"\n",
    "--job-id {training_job_name} \n",
    "--data-processing-id {processing_job_name} \n",
    "--instance-type ml.c5.xlarge\n",
    "--s3-output-uri {str(s3_bucket_uri)}/training \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%neptune_ml training start --wait --store-to training_results {training_params}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an inference endpoint in Amazon SageMaker\n",
    "\n",
    "Now that the graph representation is learned, we can deploy the learned model behind an endpoint to perform inference requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_params=f\"\"\"\n",
    "--job-id {training_job_name} \n",
    "--model-job-id {training_job_name}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%neptune_ml endpoint create --wait --store-to endpoint_results {endpoint_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name=endpoint_results['endpoint']['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the machine learning model using Gremlin\n",
    "\n",
    "Once the endpoint is ready, we can use it for graph inference queries. In our example, we can now check the friends recommendation with Neptune ML on User “Henry”. It requires almost the exact same syntax to traverse the edge, and list the other User that are connected to Henry through FRIEND connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%gremlin\n",
    "g.with(\"Neptune#ml.endpoint\",\"${endpoint_name}\").\n",
    "      V().hasLabel('User').has('name', 'Henry').\n",
    "        out('FRIEND').with(\"Neptune#ml.prediction\").hasLabel('User').values('name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is another sample prediction query, used to predict the top eight users that are most likely to connect with Henry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%gremlin\n",
    "g.with(\"Neptune#ml.endpoint\",\"${endpoint_name}\").with(\"Neptune#ml.limit\",8).\n",
    "    V().hasLabel('User').has('name', 'Henry').out('FRIEND').with(\"Neptune#ml.prediction\").hasLabel('User').values('name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the endpoint \n",
    "\n",
    "Now that you have completed this walkthrough you have created a Sagemaker endpoint which is currently running and will incur the standard charges.  If you are done trying out Neptune ML and would like to avoid these recurring costs, run the cell below to delete the inference endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "sm_boto3 = boto3.client('sagemaker')\n",
    "sm_boto3.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model transform or retraining when graph data changed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the scenarios where you have continuously changing graphs, you may need to update ML predictions with the newest graph data. The generated model artifacts after training are directly tied to the training graph which means that the inference endpoint needs to be updated once the entities in the original training graph changes. \n",
    "\n",
    "However, you don’t need to retrain the whole model in order to make predictions on the updated graph. With incremental model inference workflow, you only need to export the data from Neptune DB, incremental data preprocessing, model transform and update the inference endpoint. The model-transform step takes the trained model from the main workflow and the results of the incremental data preprocessing step as inputs, and output new model artifact to use for inference. This new model artifact has the up-to-date graph. \n",
    "\n",
    "See more Neptune ML implementation details at [Generating new model artifacts](https://docs.aws.amazon.com/neptune/latest/userguide/machine-learning-model-artifacts.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
